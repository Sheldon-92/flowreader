#!/usr/bin/env node

/**
 * Personalization A/B Testing Script
 * Privacy-first personalization experiments with statistical analysis
 */

import { readFileSync, writeFileSync, existsSync } from 'fs';
import { join } from 'path';

// Types for the script
interface ExperimentResult {
  experimentId: string;
  variantId: string;
  metrics: {
    satisfaction: number[];
    engagement: number[];
    taskCompletion: number[];
    featureDiscovery: number[];
    knowledgeUsage: number[];
    errorRate: number[];
  };
  sampleSize: number;
  startDate: string;
  endDate?: string;
}

interface StatisticalAnalysis {
  experimentId: string;
  comparison: VariantComparison[];
  overallSignificance: boolean;
  recommendations: string[];
  confidenceLevel: number;
  effect_sizes: Record<string, number>;
  sample_size_adequacy: boolean;
}

interface VariantComparison {
  metric: string;
  control: VariantStats;
  treatment: VariantStats;
  pValue: number;
  significant: boolean;
  effectSize: number;
  confidenceInterval: [number, number];
}

interface VariantStats {
  mean: number;
  stdDev: number;
  sampleSize: number;
  variance: number;
}

class PersonalizationExperimentAnalyzer {
  private dryRun: boolean = false;
  private verbose: boolean = false;

  constructor(options: { dryRun?: boolean; verbose?: boolean } = {}) {
    this.dryRun = options.dryRun || false;
    this.verbose = options.verbose || false;
  }

  /**
   * Run A/B analysis for personalization experiments
   */
  async runAnalysis(experimentId?: string): Promise<void> {
    console.log('üß™ FlowReader Personalization A/B Testing Analysis\\n');

    if (this.dryRun) {
      console.log('üü° DRY RUN MODE - No actual changes will be made\\n');
    }

    try {
      // Load experiment data
      const experiments = await this.loadExperimentData(experimentId);

      if (experiments.length === 0) {
        console.log('‚ùå No experiment data found');
        return;
      }

      // Analyze each experiment
      for (const experiment of experiments) {
        await this.analyzeExperiment(experiment);
      }

      console.log('\\n‚úÖ Analysis complete!');

    } catch (error) {
      console.error('‚ùå Analysis failed:', error);
      process.exit(1);
    }
  }

  /**
   * Analyze a single experiment
   */
  private async analyzeExperiment(experiment: ExperimentResult): Promise<void> {
    console.log(`\\nüìä Analyzing experiment: ${experiment.experimentId}`);
    console.log(`   Sample size: ${experiment.sampleSize}`);
    console.log(`   Duration: ${experiment.startDate} - ${experiment.endDate || 'ongoing'}`);

    // Generate synthetic data for demonstration (in real implementation, load from database)
    const results = this.generateAnalysisData(experiment);

    // Perform statistical analysis
    const analysis = this.performStatisticalAnalysis(results);

    // Print results
    this.printAnalysisResults(analysis);

    // Generate recommendations
    const recommendations = this.generateRecommendations(analysis);
    this.printRecommendations(recommendations);

    // Save results if not dry run
    if (!this.dryRun) {
      await this.saveAnalysisResults(analysis);
    }
  }

  /**
   * Generate analysis data (placeholder for real data loading)
   */
  private generateAnalysisData(experiment: ExperimentResult): {
    control: ExperimentResult;
    treatment: ExperimentResult;
  } {
    // Generate realistic but synthetic data for demonstration
    const control: ExperimentResult = {
      experimentId: experiment.experimentId,
      variantId: 'control',
      sampleSize: Math.floor(experiment.sampleSize * 0.5),
      startDate: experiment.startDate,
      endDate: experiment.endDate,
      metrics: {
        satisfaction: this.generateMetricData(4.1, 0.8, Math.floor(experiment.sampleSize * 0.5)),
        engagement: this.generateMetricData(180, 45, Math.floor(experiment.sampleSize * 0.5)), // seconds
        taskCompletion: this.generateMetricData(0.78, 0.15, Math.floor(experiment.sampleSize * 0.5)), // rate
        featureDiscovery: this.generateMetricData(0.42, 0.20, Math.floor(experiment.sampleSize * 0.5)), // rate
        knowledgeUsage: this.generateMetricData(3.2, 1.8, Math.floor(experiment.sampleSize * 0.5)), // requests per session
        errorRate: this.generateMetricData(0.08, 0.03, Math.floor(experiment.sampleSize * 0.5)) // rate
      }
    };

    const treatment: ExperimentResult = {
      experimentId: experiment.experimentId,
      variantId: 'personalized_basic',
      sampleSize: Math.floor(experiment.sampleSize * 0.5),
      startDate: experiment.startDate,
      endDate: experiment.endDate,
      metrics: {
        satisfaction: this.generateMetricData(4.52, 0.7, Math.floor(experiment.sampleSize * 0.5)), // Higher satisfaction
        engagement: this.generateMetricData(207, 38, Math.floor(experiment.sampleSize * 0.5)), // 15% more engagement
        taskCompletion: this.generateMetricData(0.86, 0.12, Math.floor(experiment.sampleSize * 0.5)), // 10% better completion
        featureDiscovery: this.generateMetricData(0.50, 0.18, Math.floor(experiment.sampleSize * 0.5)), // 20% more discovery
        knowledgeUsage: this.generateMetricData(4.0, 2.1, Math.floor(experiment.sampleSize * 0.5)), // 25% more usage
        errorRate: this.generateMetricData(0.056, 0.025, Math.floor(experiment.sampleSize * 0.5)) // 30% fewer errors
      }
    };

    return { control, treatment };
  }

  /**
   * Generate metric data with normal distribution
   */
  private generateMetricData(mean: number, stdDev: number, sampleSize: number): number[] {
    const data: number[] = [];
    for (let i = 0; i < sampleSize; i++) {
      // Box-Muller transformation for normal distribution
      const u1 = Math.random();
      const u2 = Math.random();
      const z0 = Math.sqrt(-2 * Math.log(u1)) * Math.cos(2 * Math.PI * u2);
      const value = mean + z0 * stdDev;
      data.push(Math.max(0, value)); // Ensure non-negative values
    }
    return data;
  }

  /**
   * Perform statistical analysis
   */
  private performStatisticalAnalysis(results: {
    control: ExperimentResult;
    treatment: ExperimentResult;
  }): StatisticalAnalysis {
    const { control, treatment } = results;
    const comparisons: VariantComparison[] = [];

    // Analyze each metric
    const metrics = ['satisfaction', 'engagement', 'taskCompletion', 'featureDiscovery', 'knowledgeUsage', 'errorRate'] as const;

    for (const metric of metrics) {
      const controlData = control.metrics[metric];
      const treatmentData = treatment.metrics[metric];

      const controlStats = this.calculateStats(controlData);
      const treatmentStats = this.calculateStats(treatmentData);

      // Perform t-test
      const tTestResult = this.performTTest(controlData, treatmentData);

      // Calculate effect size (Cohen's d)
      const pooledStd = Math.sqrt(
        ((controlStats.sampleSize - 1) * controlStats.variance +
         (treatmentStats.sampleSize - 1) * treatmentStats.variance) /
        (controlStats.sampleSize + treatmentStats.sampleSize - 2)
      );
      const effectSize = (treatmentStats.mean - controlStats.mean) / pooledStd;

      // Calculate confidence interval for difference in means
      const se = Math.sqrt(
        controlStats.variance / controlStats.sampleSize +
        treatmentStats.variance / treatmentStats.sampleSize
      );
      const df = controlStats.sampleSize + treatmentStats.sampleSize - 2;
      const tCritical = 1.96; // Approximation for 95% CI
      const meanDiff = treatmentStats.mean - controlStats.mean;
      const marginOfError = tCritical * se;
      const confidenceInterval: [number, number] = [
        meanDiff - marginOfError,
        meanDiff + marginOfError
      ];

      comparisons.push({
        metric,
        control: controlStats,
        treatment: treatmentStats,
        pValue: tTestResult.pValue,
        significant: tTestResult.pValue < 0.05,
        effectSize,
        confidenceInterval
      });
    }

    // Calculate overall significance
    const significantMetrics = comparisons.filter(c => c.significant);
    const overallSignificance = significantMetrics.length >= 2; // At least 2 significant metrics

    // Generate effect sizes summary
    const effect_sizes = comparisons.reduce((acc, comp) => {
      acc[comp.metric] = comp.effectSize;
      return acc;
    }, {} as Record<string, number>);

    // Check sample size adequacy
    const minSampleSize = 500; // Minimum for reliable results
    const sample_size_adequacy = control.sampleSize >= minSampleSize && treatment.sampleSize >= minSampleSize;

    return {
      experimentId: control.experimentId,
      comparison: comparisons,
      overallSignificance,
      recommendations: [],
      confidenceLevel: 0.95,
      effect_sizes,
      sample_size_adequacy
    };
  }

  /**
   * Calculate descriptive statistics
   */
  private calculateStats(data: number[]): VariantStats {
    const n = data.length;
    const mean = data.reduce((sum, val) => sum + val, 0) / n;
    const variance = data.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / (n - 1);
    const stdDev = Math.sqrt(variance);

    return {
      mean,
      stdDev,
      sampleSize: n,
      variance
    };
  }

  /**
   * Perform Welch's t-test
   */
  private performTTest(sample1: number[], sample2: number[]): { tStat: number; pValue: number } {
    const stats1 = this.calculateStats(sample1);
    const stats2 = this.calculateStats(sample2);

    // Welch's t-test
    const se = Math.sqrt(
      stats1.variance / stats1.sampleSize +
      stats2.variance / stats2.sampleSize
    );

    const tStat = (stats2.mean - stats1.mean) / se;

    // Degrees of freedom for Welch's t-test
    const df = Math.pow(
      stats1.variance / stats1.sampleSize + stats2.variance / stats2.sampleSize,
      2
    ) / (
      Math.pow(stats1.variance / stats1.sampleSize, 2) / (stats1.sampleSize - 1) +
      Math.pow(stats2.variance / stats2.sampleSize, 2) / (stats2.sampleSize - 1)
    );

    // Approximate p-value calculation (simplified)
    const pValue = this.calculatePValue(Math.abs(tStat), df);

    return { tStat, pValue };
  }

  /**
   * Approximate p-value calculation
   */
  private calculatePValue(tStat: number, df: number): number {
    // Simplified p-value calculation
    // In a real implementation, use a proper statistical library
    if (tStat > 2.576) return 0.01;   // 99% confidence
    if (tStat > 1.96) return 0.05;    // 95% confidence
    if (tStat > 1.645) return 0.10;   // 90% confidence
    return 0.20; // Not significant
  }

  /**
   * Print analysis results
   */
  private printAnalysisResults(analysis: StatisticalAnalysis): void {
    console.log('\\nüìà Statistical Analysis Results:');
    console.log(`   Overall significance: ${analysis.overallSignificance ? '‚úÖ YES' : '‚ùå NO'}`);
    console.log(`   Sample size adequate: ${analysis.sample_size_adequacy ? '‚úÖ YES' : '‚ö†Ô∏è  NO'}`);
    console.log(`   Confidence level: ${(analysis.confidenceLevel * 100).toFixed(1)}%\\n`);

    // Print metric comparisons
    console.log('üìä Metric Comparisons:');
    for (const comp of analysis.comparison) {
      const improvement = ((comp.treatment.mean - comp.control.mean) / comp.control.mean * 100);
      const significanceIcon = comp.significant ? '‚úÖ' : '‚ùå';

      console.log(`   ${comp.metric}:`);
      console.log(`     Control:   ${comp.control.mean.toFixed(3)} ¬± ${comp.control.stdDev.toFixed(3)}`);
      console.log(`     Treatment: ${comp.treatment.mean.toFixed(3)} ¬± ${comp.treatment.stdDev.toFixed(3)}`);
      console.log(`     Change:    ${improvement > 0 ? '+' : ''}${improvement.toFixed(1)}% ${significanceIcon}`);
      console.log(`     P-value:   ${comp.pValue.toFixed(4)}`);
      console.log(`     Effect:    ${comp.effectSize.toFixed(3)} (${this.interpretEffectSize(comp.effectSize)})`);
      console.log(`     95% CI:    [${comp.confidenceInterval[0].toFixed(3)}, ${comp.confidenceInterval[1].toFixed(3)}]\\n`);
    }
  }

  /**
   * Interpret effect size
   */
  private interpretEffectSize(effectSize: number): string {
    const abs = Math.abs(effectSize);
    if (abs < 0.2) return 'negligible';
    if (abs < 0.5) return 'small';
    if (abs < 0.8) return 'medium';
    return 'large';
  }

  /**
   * Generate recommendations
   */
  private generateRecommendations(analysis: StatisticalAnalysis): string[] {
    const recommendations: string[] = [];

    // Primary satisfaction metric check
    const satisfactionComp = analysis.comparison.find(c => c.metric === 'satisfaction');
    if (satisfactionComp) {
      const satisfactionTarget = 4.5;
      if (satisfactionComp.treatment.mean >= satisfactionTarget && satisfactionComp.significant) {
        recommendations.push(`üéØ PRIMARY GOAL ACHIEVED: Satisfaction reached ${satisfactionComp.treatment.mean.toFixed(2)} (target: ${satisfactionTarget})`);
        recommendations.push('‚úÖ RECOMMENDATION: Proceed with gradual rollout to larger population');
      } else if (satisfactionComp.treatment.mean < satisfactionTarget) {
        recommendations.push(`‚ö†Ô∏è  PRIMARY GOAL NOT MET: Satisfaction is ${satisfactionComp.treatment.mean.toFixed(2)} (target: ${satisfactionTarget})`);
        recommendations.push('üîß RECOMMENDATION: Analyze user feedback and refine personalization algorithms');
      }
    }

    // Sample size recommendations
    if (!analysis.sample_size_adequacy) {
      recommendations.push('‚ö†Ô∏è  SAMPLE SIZE: Current sample size may be insufficient for reliable conclusions');
      recommendations.push('üìä RECOMMENDATION: Continue experiment until reaching minimum 500 users per variant');
    }

    // Performance recommendations
    const errorComp = analysis.comparison.find(c => c.metric === 'errorRate');
    if (errorComp && errorComp.treatment.mean > errorComp.control.mean) {
      recommendations.push('üö® PERFORMANCE CONCERN: Error rate increased in treatment group');
      recommendations.push('üîß RECOMMENDATION: Investigate and fix performance issues before proceeding');
    }

    // Overall recommendation
    if (analysis.overallSignificance && analysis.sample_size_adequacy) {
      const positiveMetrics = analysis.comparison.filter(c =>
        c.significant && c.treatment.mean > c.control.mean && c.metric !== 'errorRate'
      ).length;

      if (positiveMetrics >= 3) {
        recommendations.push('üöÄ OVERALL RECOMMENDATION: Strong positive results - proceed with rollout');
      } else if (positiveMetrics >= 2) {
        recommendations.push('‚úÖ OVERALL RECOMMENDATION: Positive results - consider gradual expansion');
      } else {
        recommendations.push('‚ö†Ô∏è  OVERALL RECOMMENDATION: Mixed results - analyze further before expansion');
      }
    }

    return recommendations;
  }

  /**
   * Print recommendations
   */
  private printRecommendations(recommendations: string[]): void {
    if (recommendations.length === 0) return;

    console.log('\\nüí° Recommendations:');
    for (const rec of recommendations) {
      console.log(`   ${rec}`);
    }
  }

  /**
   * Load experiment data
   */
  private async loadExperimentData(experimentId?: string): Promise<ExperimentResult[]> {
    // In a real implementation, this would load from a database
    // For demonstration, return synthetic data

    const experiments: ExperimentResult[] = [
      {
        experimentId: 'personalization_rollout_v1',
        variantId: 'control',
        sampleSize: 1200,
        startDate: '2025-01-15T00:00:00Z',
        endDate: '2025-02-15T00:00:00Z',
        metrics: {
          satisfaction: [],
          engagement: [],
          taskCompletion: [],
          featureDiscovery: [],
          knowledgeUsage: [],
          errorRate: []
        }
      }
    ];

    if (experimentId) {
      return experiments.filter(e => e.experimentId === experimentId);
    }

    return experiments;
  }

  /**
   * Save analysis results
   */
  private async saveAnalysisResults(analysis: StatisticalAnalysis): Promise<void> {
    const resultsPath = join(process.cwd(), 'analysis_results');
    const filename = `personalization_${analysis.experimentId}_${new Date().toISOString().split('T')[0]}.json`;

    console.log(`\\nüíæ Saving results to: ${filename}`);

    // In a real implementation, save to database and/or file system
    if (this.verbose) {
      console.log('   Analysis data:', JSON.stringify(analysis, null, 2));
    }
  }
}

// CLI Interface
async function main() {
  const args = process.argv.slice(2);
  const dryRun = args.includes('--dry-run');
  const verbose = args.includes('--verbose') || args.includes('-v');
  const experimentId = args.find(arg => arg.startsWith('--experiment-id='))?.split('=')[1];

  const analyzer = new PersonalizationExperimentAnalyzer({ dryRun, verbose });
  await analyzer.runAnalysis(experimentId);
}

// Help function
function printHelp() {
  console.log(`
FlowReader Personalization A/B Testing Script

Usage:
  node scripts/experiments/personalization_ab.ts [options]

Options:
  --dry-run                Run analysis without making changes
  --verbose, -v           Show detailed output
  --experiment-id=ID      Analyze specific experiment
  --help, -h             Show this help message

Examples:
  # Analyze all personalization experiments
  node scripts/experiments/personalization_ab.ts

  # Dry run analysis for specific experiment
  node scripts/experiments/personalization_ab.ts --dry-run --experiment-id=personalization_rollout_v1

  # Verbose analysis
  node scripts/experiments/personalization_ab.ts --verbose
`);
}

// Handle help and run
if (process.argv.includes('--help') || process.argv.includes('-h')) {
  printHelp();
  process.exit(0);
} else {
  main().catch(console.error);
}

export { PersonalizationExperimentAnalyzer };
export type { ExperimentResult, StatisticalAnalysis, VariantComparison };