# Story 3.1: AI Reading Companion

## Status
✅ **Approved** - Ready for Development

## Story

**As a** reader engaging with complex or unfamiliar content,  
**I want** to ask questions and have conversations with an AI that understands the book I'm reading,  
**so that** I can deepen my understanding, explore ideas, and get immediate clarification without leaving the reading experience.

## Acceptance Criteria

1. **Contextual AI Access**
   - User can invoke AI assistant from any point in the reading interface
   - AI has full context of the book being read and user's current position
   - AI can reference specific passages, characters, themes from the entire book
   - Conversation interface appears without disrupting the reading flow

2. **Intelligent Conversation Quality**
   - AI provides thoughtful, relevant responses about book content
   - AI can explain complex concepts, analyze themes, and discuss character motivations
   - AI adapts response complexity to match the book's level and user's questions
   - AI maintains conversation context across multiple exchanges

3. **Text Selection Integration**
   - User can select specific text passages to ask questions about
   - AI references the exact selected text in its responses
   - AI can compare selected passage with other parts of the book
   - Selected text context is clearly displayed in the conversation

4. **Quota Management & Performance**
   - AI responses use streaming display with first token <3s, complete response varies by complexity
   - Clear indication of AI usage quota and remaining capacity
   - Graceful handling of quota limits with option to purchase more
   - Conversation continues smoothly even when approaching limits

5. **AI Cost Circuit-Breaker Protection**
   - Hard stop at $100/month per Pro user, $5/month per trial user for AI API costs
   - Real-time token estimation with user confirmation for requests >10K tokens (>$0.50)
   - Automatic AI service suspension when cost limits approached (at 90% threshold)
   - Admin alerts triggered when any user exceeds 80% of their AI cost limit
   - Context optimization to minimize token usage while maintaining conversation quality

6. **Conversation Persistence**
   - All AI conversations are saved and linked to the specific book
   - User can review previous conversations and pick up where they left off
   - Conversations are searchable within the book's context
   - Conversation history persists across devices and sessions

7. **Multiple Conversation Types**
   - **Explanation**: "What does this passage mean?"
   - **Analysis**: "What's the significance of this scene?"
   - **Brainstorming**: "What if the character had chosen differently?"
   - **Synthesis**: "How does this connect to themes in other chapters?"

## Tasks / Subtasks

- [ ] **AI Service Integration** (AC: 1, 2, 4)
  - [ ] Integrate Google Gemini API with long-context capabilities
  - [ ] Implement secure API key management in Edge Functions
  - [ ] Create book context preparation and chunking strategies
  - [ ] Build conversation context management with token optimization
  - [ ] Add response streaming for better user experience

- [ ] **Contextual AI Framework** (AC: 1, 2)
  - [ ] Create book content indexing for AI context
  - [ ] Implement current reading position context injection
  - [ ] Build character, theme, and plot tracking system
  - [ ] Create book-specific prompt engineering templates
  - [ ] Add conversation memory and context threading

- [ ] **Conversation Interface** (AC: 1, 3)
  - [ ] Design AI chat modal/sidebar component
  - [ ] Build text selection → AI question workflow
  - [ ] Create conversation bubble UI with proper formatting
  - [ ] Implement typing indicators and loading states
  - [ ] Add conversation controls (clear, restart, save)

- [ ] **Text Selection & Reference System** (AC: 3)
  - [ ] Implement text selection highlighting and capture
  - [ ] Build selected text context display in AI interface
  - [ ] Create reference links between AI responses and text passages
  - [ ] Add quote/citation formatting for AI references
  - [ ] Handle text selection across different book formats

- [ ] **Quota & Usage Management** (AC: 4, 5)
  - [ ] Implement AI token usage tracking
  - [ ] Create quota checking before conversation requests
  - [ ] Build usage display and warning system
  - [ ] Add quota exceeded handling with upgrade options
  - [ ] Implement conversation length optimization

- [ ] **AI Cost Circuit-Breaker System** (AC: 5)
  - [ ] Implement real-time token cost calculation per request
  - [ ] Create hard limit enforcement at user tier limits
  - [ ] Build AI cost warning system (80%, 90% thresholds)
  - [ ] Add automatic AI service suspension mechanism
  - [ ] Create admin cost monitoring and alert system
  - [ ] Implement context optimization for token usage reduction

- [ ] **Conversation Persistence & Management** (AC: 5)
  - [ ] Create AI conversation database schema and storage
  - [ ] Build conversation history UI and navigation
  - [ ] Implement conversation search functionality
  - [ ] Add conversation organization and tagging
  - [ ] Create cross-device conversation synchronization

- [ ] **Conversation Type Specialization** (AC: 6)
  - [ ] Build specialized prompts for different conversation types
  - [ ] Create conversation mode selection interface
  - [ ] Implement context-aware response formatting
  - [ ] Add conversation type indicators and organization
  - [ ] Build response quality optimization for each type

## Dev Notes

### Architecture Context
- AI companion is a core differentiating feature leveraging Google Gemini's long-context capabilities
- Integrates with existing book reading infrastructure and user management
- Requires sophisticated context management to provide book-aware responses
- Must balance response quality with token usage costs

### Key Technical Specifications
- **AI Provider**: Google Gemini Pro with 1M+ token context window
- **Context Strategy**: Book content + reading position + conversation history
- **Token Management**: Sliding window approach with context caching
- **Response Streaming**: Server-sent events for real-time response display
- **Quota Limits**: 75K tokens/month for Pro tier, 10K for trial

### Integration Points
- Uses existing book content from EPUB/TXT processing
- Integrates with text selection system from reading interface  
- Connects with user authentication and quota management
- Links with note-taking and knowledge capture systems

### AI Context Engineering
- **Book Context**: Layered approach - current chapter + relevant sections via retrieval (not full book)
- **Conversation Context**: Previous exchanges, conversation type, user intent
- **Reading Context**: Current chapter, recent passages, selected text
- **User Context**: Reading history, preferences, previous conversations
- **Privacy & Compliance**: User consent for book content transmission to AI service
- **Content Filtering**: Prompt injection prevention and sensitive content handling

### Cost Management Strategies
1. **Context Caching**: Cache book content context to reduce token usage
2. **Context Pruning**: Remove old conversation context while preserving relevance
3. **Response Optimization**: Encourage concise but comprehensive responses
4. **Batch Processing**: Group multiple questions when possible

### Technical Implementation Details
- **API Integration**: Edge Functions proxy with request/response processing
- **Context Preparation**: Client-side text preparation, server-side context injection
- **Token Estimation**: Pre-flight token counting for quota enforcement
- **Error Handling**: Graceful degradation for API failures or quota exceeded

### Testing

**Test Coverage Requirements**:
- Unit tests for context preparation and token management
- Integration tests for Gemini API interaction and response processing
- E2E tests for complete AI conversation workflows
- Performance tests for response time and context loading

**Testing Standards**:
- Mock Gemini API responses for predictable testing
- Use test books with known content for context verification
- Conversation quality assessment through manual and automated testing
- Load testing for concurrent AI conversations

**Specific Test Cases**:
- AI conversation about specific book passages with accurate context
- Text selection → AI question workflow with proper reference handling
- Conversation persistence across sessions and devices
- Quota enforcement (approaching limit, at limit, exceeded)
- Different conversation types (explanation, analysis, brainstorming)
- Long conversation context management and token optimization
- Network failure and API error handling
- Cross-book conversation isolation (no context bleeding)

### Response Quality Metrics
- **Accuracy**: AI responses correctly reference book content
- **Relevance**: Responses directly address user questions
- **Depth**: Responses provide meaningful analysis and insight
- **Consistency**: AI maintains consistent understanding across conversations
- **Speed**: <10s response time for typical questions

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-09-08 | 1.0 | Initial story creation with comprehensive AI companion requirements | Sarah (PO) |

## Dev Agent Record

*This section will be populated during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results

*This section will be populated after QA review*

---

## Addendum · SSE Contract & RAG Details (2025-09-10)

### Runtime & Protocol

- 使用 Vercel Serverless Functions（Node）以便SSE与SDK兼容；`text/event-stream` + 心跳保持连接。
- 连接心跳≤15s；客户端支持`Last-Event-ID`重连。

### SSE Endpoint

- POST `/api/chat/stream` → SSE
  - Request: `{ book_id: string, chapter_idx?: number, selection: string, conversation_id?: string }`
  - Events:
    - `sources` → `[{ chapter_idx, start, end }]`
    - `token` → `"...chunk..."`
    - `usage` → `{ prompt_tokens, completion_tokens, cost_usd }`
    - `done`
    - `error` → `{ code, message }`

### Retrieval (RAG)

- 表：`chapter_embeddings(id, book_id, chapter_idx, chunk_id, embedding vector, start, end)`。
- 策略：章内top-k（阈值≥τ）→不足时全书补齐；k、τ可配置；返回引用范围。
- 会话摘要/截断：超过窗口进行摘要并保留最近轮次。

### Quota · Rate Limit · Errors

- 配额：月度token与日请求上限；触发返回`429_QUOTA`；速率限制返回`429_RATE_LIMITED`。
- 错误码：`QUOTA_EXCEEDED`, `RATE_LIMITED`, `SSE_ABORTED`, `INTERNAL_ERROR`。

### Analytics

- `chat_request { book_id, selection_len }`
- `chat_first_token { ttft_ms }`
- `chat_complete { tokens }`
- `quota_block | rate_limited`

### NFR Targets（增补）

- TTFT ≤ 2s；p95生成中断率 < 1%；弱网自动重连≤3次后提示一次性回答。
